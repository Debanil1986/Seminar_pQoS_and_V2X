import json
import pandas as pd
import numpy as np

def preprocess_single_file(file_path):
    # Load JSON data
    with open(file_path, 'r') as f:
        data = json.load(f)

    # Initialize data containers
    transforms_data = []
    objects_data = []
    occlusion_map = {
      'NOT_OCCLUDED': 0.29,
      'PARTIALLY_OCCLUDED': 0.64,
      'MOSTLY_OCCLUDED': 0.98,
      '': 0.2  # default for blank or missing
    }

    # Extract transforms
    for frame in data['openlabel']['frames'].values():
        transforms = frame['frame_properties']['transforms']
        for tf_name, tf_info in transforms.items():
            transforms_data.append({
                'source_sensor': tf_info['src'],
                'target_sensor': tf_info['dst'],
                'transformation_matrix': np.array(tf_info['transform_src_to_dst']['matrix4x4'])
            })

    # Extract objects
    for frame_id, frame_data in data['openlabel']['frames'].items():
        for obj_id, obj_info in frame_data['objects'].items():
            cuboid = obj_info['object_data']['cuboid']['val']
            attributes = obj_info['object_data']['cuboid']['attributes']

            # Parse attributes
            attr_dict = {'sensor_id': None, 'occlusion_level': None, 'num_points': -1, 'score': -1}
            for text_attr in attributes['text']:
                if text_attr['name'] == 'sensor_id':
                    attr_dict['sensor_id'] = text_attr['val']
                if text_attr['name'] == 'occlusion_level':
                    attr_dict['occlusion_level'] = text_attr['val']
            for num_attr in attributes['num']:
                if num_attr['name'] == 'num_points':
                    attr_dict['num_points'] = num_attr['val']
                if num_attr['name'] == 'score':
                    occlusion_factor = occlusion_map.get(attr_dict['occlusion_level'], 0.2)
                    density_score_calculation = (attr_dict['num_points'] / (cuboid[7] * cuboid[8] * cuboid[9])) * (1- occlusion_factor )
                    attr_dict['score'] = num_attr['val'] if num_attr['val'] > 0 else density_score_calculation

            # Parse cuboid values (assuming format: [x, y, z, qx, qy, qz, qw, length, width, height])
            objects_data.append({
                'frame_id': frame_id,
                'object_id': obj_id,
                'object_type': obj_info['object_data']['type'],
                'x': cuboid[0],
                'y': cuboid[1],
                'z': cuboid[2],
                'qx': cuboid[3],
                'qy': cuboid[4],
                'qz': cuboid[5],
                'qw': cuboid[6],
                'length': cuboid[7],
                'width': cuboid[8],
                'height': cuboid[9],
                **attr_dict
            })

    # Create DataFrames
    transforms_df = pd.DataFrame(transforms_data)
    objects_df = pd.DataFrame(objects_data)

    # Add timestamp
    timestamp = next(iter(data['openlabel']['frames'].values()))['frame_properties']['timestamp']
    objects_df['timestamp'] = timestamp

    return transforms_df, objects_df


def process_multiple_files(file_list):
    all_transforms = []
    all_objects = []
    
    for file_path in file_list:
        t_df, o_df = preprocess_single_file(file_path)
        all_transforms.append(t_df)
        all_objects.append(o_df)
    
    return pd.concat(all_transforms), pd.concat(all_objects)



file_list = [f"LIDARJSONS/preprocessLidar_{i:02}.json" for i in range(1,28)]
transforms_full, objects_full = process_multiple_files(file_list)





newData= objects_full.sort_values("timestamp")
n = len(objects_full)
train = objects_full.iloc[:int(0.6*n)]
val = objects_full.iloc[int(0.6*n):int(0.8*n)]
test = objects_full.iloc[int(0.8*n):]


train.to_csv("train_new.csv", index=False)
print(f"Train of size: {len(train)} is added")
val.to_csv("val_new.csv", index=False)
print(f"Validation of size: {len(val)} is added")
test.to_csv("test_new.csv", index=False)
print(f"Test of size: {len(test)} is added")


transforms_full.head(10)
